# Key Deep Learning Architectures

This repository provides overview of some of the prominent neural network architectures.

### List of Architectures

Partially this list comes from official [Keras applications](https://keras.io/applications/). Sorted in chronological order:

- LeNet, 1998, [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)
- AlexNet, 2012, [paper](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
- GoogLeNet, 2014, [paper](https://arxiv.org/abs/1409.4842)
- Inception, 2014, [paper](https://arxiv.org/pdf/1409.4842v1.pdf), "Going deeper with convolutions"
- VGG, 2014, [paper](https://arxiv.org/abs/1409.1556)
- InceptionV2, InceptionV3, 2016, [paper](https://arxiv.org/abs/1512.00567)
- ResNet, 2015, [paper](https://arxiv.org/abs/1512.03385)
- InceptionV4, 2016, [paper](https://arxiv.org/abs/1602.07261)
- InceptionResNetV2, [paper](https://arxiv.org/abs/1602.07261) (same paper as InceptionV4)
- DenseNet, 2016, [paper](https://arxiv.org/abs/1608.06993)
- Xception, 2016, [paper](https://arxiv.org/abs/1610.02357)
- MobileNet, 2017, [paper](https://arxiv.org/abs/1704.04861)
- NASNet, 2017, [paper](https://arxiv.org/pdf/1707.07012.pdf) | [blogpost](https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html)
- MobileNetV2, 2018, [paper](https://arxiv.org/abs/1801.04381)

----

- Faster R-CNN, 2015, [paper](https://arxiv.org/abs/1506.01497)
- xxx [paper]()


## LeNet-5 [1998, [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf) by LeCun et al.]

![LeNet-5](/images/lenet-5.png)

**Main ideas**: local receptive fields, shared weights, spacial subsampling

**Why it is important**: LeNet-5 was used on large scale to automatically classify hand-written digits on bank cheques in the United States. This network is a convolutional neural network (CNNs). CNNs introduced in the paper are the foundation of modern state-of-the art deep learning. CNNs are built upon 3 main ideas: local receptive fields, shared weights and spacial subsampling. Local receptive fields with shared weights are the essence of the convolutional layer and most [ALL?] architectures described below use convolutional layers in one form or another. 

**Brief description**: 

**Additional readings**:


## XXX [20xx, [paper]() by XXX et al.]

![xxx](/images/xxx.png)

**Main ideas**: 

**Why it is important**: 

**Brief description**:

**Additional readings**: